{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_Rome.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bTSNbWwf_tkY",
        "FZ63lS-4iV0q",
        "_LHvej4KWyc1",
        "RVjJ8VfZAAD4",
        "4PHHXB8L4Sxi",
        "JLG6u240XlT3",
        "pnLlBbVkHSOT",
        "ivyGLh6ctYNv",
        "t_DWjYAwqZV0",
        "vOwRpkARfH42",
        "H4AFYQgVKafR",
        "SRiyGwXlV6Sm",
        "H1EEeJC37BI-"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPeBgQDX-YyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = \"/content/drive/My Drive\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTSNbWwf_tkY",
        "colab_type": "text"
      },
      "source": [
        "# **LOADING PACKAGES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks0AHhZQ-kTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install geopandas\n",
        "!apt-get install libproj-dev proj-data proj-bin\n",
        "!apt-get install libgeos-dev\n",
        "!pip install cython\n",
        "!pip install contextily\n",
        "import math\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import contextily as ctx\n",
        "import timeit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gExtr_u3uVtK",
        "colab_type": "text"
      },
      "source": [
        "# **PREPROCESSING: CONVERT TEMPORAL DATA TO L-SPACE EDGE- LIST OF  SELECTED TIMESPAN**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abv7wamXkWMR",
        "colab_type": "text"
      },
      "source": [
        "Rome 8AM - 9AM 2017-11-06"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbV7nlVjujCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The code below converts the full-day network temporal event list to an L-space\n",
        "edge list of the selected timespan (either 8AM-9AM or 11-12AM our study).\n",
        "An example of the resulting edge list is visible in Table 3 of the paper.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "combined_L =  pd.read_csv(DATA_PATH + \"/rome/network_combined.csv\", sep = \";\") #Loading the combined edge list as introduced by Kujala et al. (2018)\n",
        "data = pd.read_csv(DATA_PATH + \"/rome/network_temporal_day.csv\", sep = \";\") #Loading the network event list as introduced by Kujala et al. (2018)\n",
        "data[\"dep_time_ut\"] = pd.to_datetime(data[\"dep_time_ut\"], unit = \"s\") #transforms Unix timestamp to datetime\n",
        "data[\"arr_time_ut\"] = pd.to_datetime(data[\"arr_time_ut\"], unit = \"s\") #ransforms Unix timestamp to  datetime\n",
        "data[\"dep_time_ut\"] += pd.to_timedelta(1, unit='h') #UTC+1h  https://www.timeanddate.com/time/zone/italy/rome\n",
        "data[\"arr_time_ut\"] += pd.to_timedelta(1, unit='h') #UTC+1h  https://www.timeanddate.com/time/zone/italy/rome\n",
        "data[\"travel_time\"] = data[\"arr_time_ut\"]-data[\"dep_time_ut\"] #calculate total travel time\n",
        "data = data[(data[\"dep_time_ut\"] >= \"2017-11-06 08:00:00\") & (data[\"dep_time_ut\"] < \"2017-11-06 09:00:00\")] #selecting timespan: 8am-9am\n",
        "data[\"travel_time\"] = data[\"travel_time\"] / np.timedelta64(1, 's') #converting timedelta to seconds\n",
        "data = data.groupby(['from_stop_I', 'to_stop_I', \"route_type\",\"route_I\", \"travel_time\"]).size()\n",
        "data = data.reset_index(name=\"n_vehicles\") #Put indexes back\n",
        "data[\"travel_time\"] = data[\"travel_time\"].astype(int) #travel time value as integer\n",
        "data[\"n_vehicles\"] = data[\"n_vehicles\"].astype(int) #number of vehicles values as integer\n",
        "data.loc[:,[\"travel_time\"]]=data.loc[:,[\"travel_time\"]].values*data.loc[:,[\"n_vehicles\"]].values #travel time*n_vehicles to get the right value after the groupby function\n",
        "data = data.groupby(['from_stop_I', 'to_stop_I', \"route_type\", \"route_I\"]).agg({\"travel_time\": \"sum\",\n",
        "                                                                    \"n_vehicles\": \"sum\"}).reset_index() #merge mostly duplicate rows (i.e. same edges in the network) \n",
        "data.loc[:,[\"travel_time\"]]=data.loc[:,[\"travel_time\"]].values/data.loc[:,[\"n_vehicles\"]].values  #travel time/n_vehicles to derive the right values out of the groupby function                                                          \n",
        "data['route_I_counts'] = data[[\"route_I\", 'n_vehicles']].astype(str).apply(lambda x: ':'.join(x), axis=1) #create the route_I_counts column  \n",
        "data = data.drop(columns = [\"route_I\"]) #drop route_I\n",
        "data.loc[:,[\"travel_time\"]] = data.loc[:,[\"travel_time\"]].values*data.loc[:,[\"n_vehicles\"]].values #travel time*n_vehicles to get the right value after the groupby function\n",
        "data = data.groupby(['from_stop_I', 'to_stop_I', \"route_type\"]).agg({\"travel_time\": \"sum\",\n",
        "                                                                    \"n_vehicles\": \"sum\",\n",
        "                                                                    'route_I_counts': ','.join}).reset_index()\n",
        "data.loc[:,[\"travel_time\"]]=data.loc[:,[\"travel_time\"]].values/data.loc[:,[\"n_vehicles\"]].values #travel time/n_vehicles to derive the right values out of the groupby function\n",
        "\n",
        "for row in data.itertuples(): #adding the \"d\" values from the combined edge list to the newly developed temporal event list \n",
        "  for row1 in combined_L.itertuples():\n",
        "    if row[1] == row1[1] and row[2] == row1[2]:\n",
        "      data.at[row.Index, \"d\"] = row1[3]\n",
        "      \n",
        "data = data.rename(columns = {\"travel_time\": \"duration_avg\"}) #rename column\n",
        "data = data[[\"to_stop_I\", \"from_stop_I\", \"d\", \"duration_avg\", \"n_vehicles\", \"route_I_counts\", \"route_type\"]] #Columns back in the right order\n",
        "data.to_csv(DATA_PATH + \"/rome/combined_L_8am_9am.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMPHo91BwgSV",
        "colab_type": "text"
      },
      "source": [
        "Rome 11AM-12AM 2017-11-06"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRyeeNiNwjph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The code below converts the full-day network temporal event list to an L-space\n",
        "edge list of the selected timespan (either 8AM-9AM or 11-12AM our study).\n",
        "An example of the resulting edge list is visible in Table 3 of the paper.\n",
        "\"\"\"\n",
        "\n",
        "combined_L =  pd.read_csv(DATA_PATH + \"/rome/network_combined.csv\", sep = \";\") #Loading the combined edge list as introduced by Kujala et al. (2018)\n",
        "data = pd.read_csv(DATA_PATH + \"/rome/network_temporal_day.csv\", sep = \";\")  #Loading the full-day network event list as introduced by Kujala et al. (2018)\n",
        "data[\"dep_time_ut\"] = pd.to_datetime(data[\"dep_time_ut\"], unit = \"s\") #transforms Unix timestamp to datetime\n",
        "data[\"arr_time_ut\"] = pd.to_datetime(data[\"arr_time_ut\"], unit = \"s\") #ransforms Unix timestamp to  datetime\n",
        "data[\"dep_time_ut\"] += pd.to_timedelta(1, unit='h') #UTC+1h  https://www.timeanddate.com/time/zone/italy/rome\n",
        "data[\"arr_time_ut\"] += pd.to_timedelta(1, unit='h') #UTC+1h  https://www.timeanddate.com/time/zone/italy/rome\n",
        "data[\"travel_time\"] = data[\"arr_time_ut\"]-data[\"dep_time_ut\"] #calculate total travel time\n",
        "data = data[(data[\"dep_time_ut\"] >= \"2017-11-06 11:00:00\") & (data[\"dep_time_ut\"] < \"2017-11-06 12:00:00\")] #selecting timespan: 8am-9am\n",
        "data[\"travel_time\"] = data[\"travel_time\"] / np.timedelta64(1, 's') #converting timedelta to seconds\n",
        "data = data.groupby(['from_stop_I', 'to_stop_I', \"route_type\",\"route_I\", \"travel_time\"]).size()\n",
        "data = data.reset_index(name=\"n_vehicles\") #Put indexes back\n",
        "data[\"travel_time\"] = data[\"travel_time\"].astype(int) #travel time value as integer\n",
        "data[\"n_vehicles\"] = data[\"n_vehicles\"].astype(int) #number of vehicles values as integer\n",
        "data.loc[:,[\"travel_time\"]]=data.loc[:,[\"travel_time\"]].values*data.loc[:,[\"n_vehicles\"]].values #travel time*n_vehicles to get the right value after the groupby function\n",
        "data = data.groupby(['from_stop_I', 'to_stop_I', \"route_type\", \"route_I\"]).agg({\"travel_time\": \"sum\",\n",
        "                                                                    \"n_vehicles\": \"sum\"}).reset_index() #merge mostly duplicate rows (i.e. same edges in the network) \n",
        "data.loc[:,[\"travel_time\"]]=data.loc[:,[\"travel_time\"]].values/data.loc[:,[\"n_vehicles\"]].values  #travel time/n_vehicles to derive the right values out of the groupby function                                                          \n",
        "data['route_I_counts'] = data[[\"route_I\", 'n_vehicles']].astype(str).apply(lambda x: ':'.join(x), axis=1) #create the route_I_counts column  \n",
        "data = data.drop(columns = [\"route_I\"]) #drop route_I\n",
        "data.loc[:,[\"travel_time\"]] = data.loc[:,[\"travel_time\"]].values*data.loc[:,[\"n_vehicles\"]].values #travel time*n_vehicles to get the right value after the groupby function\n",
        "data = data.groupby(['from_stop_I', 'to_stop_I', \"route_type\"]).agg({\"travel_time\": \"sum\",\n",
        "                                                                    \"n_vehicles\": \"sum\",\n",
        "                                                                    'route_I_counts': ','.join}).reset_index()\n",
        "data.loc[:,[\"travel_time\"]]=data.loc[:,[\"travel_time\"]].values/data.loc[:,[\"n_vehicles\"]].values #travel time/n_vehicles to derive the right values out of the groupby function\n",
        "\n",
        "\n",
        "for row in data.itertuples(): #adding the \"d\" values from the combined edge list to the newly developed temporal event list \n",
        "  for row1 in combined_L.itertuples():\n",
        "    if row[1] == row1[1] and row[2] == row1[2]:\n",
        "      data.at[row.Index, \"d\"] = row1[3]\n",
        "      \n",
        "data = data.rename(columns = {\"travel_time\": \"duration_avg\"}) #rename column\n",
        "data = data[[\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"n_vehicles\", \"route_I_counts\", \"route_type\"]] #Columns back in the right order\n",
        "data.to_csv(DATA_PATH + \"/rome/combined_L_11am_12am.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXQ2gYR_xqt",
        "colab_type": "text"
      },
      "source": [
        "# **LOADING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15cK45FY_8m4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Monday 8am-9am data\n",
        "combined_L_89 = pd.read_csv(DATA_PATH + \"/rome/combined_L_8am_9am.csv\") #combined data, as processed above\n",
        "tram_L_89 = combined_L_89[combined_L_89[\"route_type\"]==0] #tram data, derived from the combined data\n",
        "tram_L_89 = tram_L_89.drop(columns = [\"route_type\"])\n",
        "rail_L_89 = combined_L_89[combined_L_89[\"route_type\"]==2] #rail data, derived from the combined data\n",
        "rail_L_89 = rail_L_89.drop(columns = [\"route_type\"])\n",
        "subway_L_89 = combined_L_89[combined_L_89[\"route_type\"]==1] #subway data, derived from the combined data\n",
        "subway_L_89 = subway_L_89.drop(columns = [\"route_type\"])\n",
        "bus_L_89 = combined_L_89[combined_L_89[\"route_type\"]==3] #bus data, derived from the combined data\n",
        "bus_L_89 = bus_L_89.drop(columns = [\"route_type\"])\n",
        "\n",
        "##Monday 8am-9am data\n",
        "combined_L_1112 = pd.read_csv(DATA_PATH + \"/rome/combined_L_11am_12am.csv\") #combined data, as processed above\n",
        "tram_L_1112 = combined_L_1112[combined_L_1112[\"route_type\"]==0] #tram data, derived from the combined data\n",
        "tram_L_1112 = tram_L_1112.drop(columns = [\"route_type\"])\n",
        "rail_L_1112 = combined_L_1112[combined_L_1112[\"route_type\"]==2] #rail data, derived from the combined data\n",
        "rail_L_1112 = rail_L_1112.drop(columns = [\"route_type\"])\n",
        "subway_L_1112 = combined_L_1112[combined_L_1112[\"route_type\"]==1] #subway data, derived from the combined data\n",
        "subway_L_1112 = subway_L_1112.drop(columns = [\"route_type\"])\n",
        "bus_L_1112 = combined_L_1112[combined_L_1112[\"route_type\"]==3] #bus data, derived from the combined data\n",
        "bus_L_1112 = bus_L_1112.drop(columns = [\"route_type\"])\n",
        "\n",
        "##Walking distance\n",
        "walking_distance = pd.read_csv(DATA_PATH + '/rome/network_walk.csv', sep = \";\") #dataset comprising walking distances\n",
        "\n",
        "##Node data\n",
        "network_nodes = pd.read_csv(DATA_PATH + \"/rome/network_nodes.csv\", sep = \";\") #node list, comprising coordinates and name of stops\n",
        "\n",
        "##GeoJSON_files\n",
        "routes_geojson = gpd.read_file(DATA_PATH + '/rome/routes.geojson') #Public transport routes in GeoJSON format.\n",
        "sections_geojson = gpd.read_file(DATA_PATH + '/rome/sections.geojson') #Each stop-to-stop section in GeoJSON format.\n",
        "stops_geojson = gpd.read_file(DATA_PATH + '/rome/stops.geojson') #Information on the nodes in GeoJSON format."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCqV2_61kNN8",
        "colab_type": "text"
      },
      "source": [
        "# **Graph Networkx**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ63lS-4iV0q",
        "colab_type": "text"
      },
      "source": [
        "# **L-Space Graph**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPUHJl-_-HJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The code below creates the L_space graph for 8am-9am interval\n",
        "combined_L_89_graph = nx.from_pandas_edgelist(combined_L_89, source = \"from_stop_I\", target = \"to_stop_I\", edge_attr = (\"duration_avg\", \"n_vehicles\", \"route_I_counts\", \"d\", \"route_type\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqlcm6k77tI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The code below creates the L_space graphs for 11am-12am interval\n",
        "combined_L_1112_graph = nx.from_pandas_edgelist(combined_L_1112, source = \"from_stop_I\", target = \"to_stop_I\", edge_attr = (\"duration_avg\", \"n_vehicles\", \"route_I_counts\", \"d\", \"route_type\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JLsNQsQZdoN",
        "colab_type": "text"
      },
      "source": [
        "# **PREPROCESSING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LHvej4KWyc1",
        "colab_type": "text"
      },
      "source": [
        "## Algorithm to create the supernode network representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_M3nVbh__Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sub_graph_detector(tram_graph, rail_graph , subway_graph, bus_graph, combined_graph):\n",
        "  \"\"\"\n",
        "  This code has been used to detect disconnected subnetworks, as a first step to\n",
        "  identify temporal or spatial artefacts in the dataset as provided by Kujala et\n",
        "  al. (2018) or as a result of preprocessing.\n",
        "  \"\"\"\n",
        "  sub_dict = {\"tram\":nx.connected_components(tram_graph), \"rail\": nx.connected_components(rail_graph),\n",
        "              \"subway\": nx.connected_components(subway_graph), \"bus\": nx.connected_components(bus_graph),\n",
        "              \"combined\": nx.connected_components(combined_graph)}\n",
        "\n",
        "  for k,v in sub_dict.items():\n",
        "    count = 0\n",
        "    print()\n",
        "    print(k)\n",
        "    for i, sg in enumerate(v):   \n",
        "      print(\"Subgraph {} has {} nodes\".format(i+1, len(sg)))\n",
        "      count+= len(sg)\n",
        "    print(\"Total number of nodes:\", count)\n",
        "    print(\"\")\n",
        "      \n",
        "sub_graph_detector(tram_L_89_graph, rail_L_89_graph , subway_L_89_graph, bus_L_89_graph, combined_L_89_graph)\n",
        "print(\"=============================================\")\n",
        "sub_graph_detector(tram_L_1112_graph, rail_L_1112_graph , subway_L_1112_graph, bus_L_1112_graph, combined_L_1112_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBnU5qKO7Hrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_cut_off_nodes(G_L):\n",
        "  \"\"\" \n",
        "  After a check in the geojson files and a validation in Google maps,\n",
        "  there appear to be three nodes as subgraph in the bus and combined data that \n",
        "  have been cut of due to the 20km radius. These can be removed with this \n",
        "  function. In addition: in the rail (and thus also combined) data of the\n",
        "  interval 11am-12am, there are three stations that are disconnected both from\n",
        "  the route they are part of as well as from other stops in walking distance,\n",
        "  due to temporal artefacts.  These have been removed too.\n",
        "  See table 4 in the paper for further explanation.\n",
        "  \"\"\"\n",
        "  node_list = []\n",
        "  for sub_graph in nx.connected_components(G_L):\n",
        "    if len(sub_graph) <5:\n",
        "      for n in sub_graph:\n",
        "        node_list.append(n)\n",
        "  for node in node_list:\n",
        "    #G_P.remove_node(node)\n",
        "    G_L.remove_node(node)\n",
        "  print(\"removed nodes\", node_list)\n",
        "\n",
        "remove_cut_off_nodes(combined_L_89_graph)\n",
        "remove_cut_off_nodes(combined_L_1112_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGSv2Cgs1oRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def identify_near_nodes(walk_distance):\n",
        "  \"\"\"\n",
        "  This function identifies the nodes that are at a distance of max the\n",
        "  determined merge_radius (and thus are going to be transformed into \n",
        "  supernodes)\n",
        "  \"\"\"\n",
        "  merge_radius = 100\n",
        "  near_nodes = []\n",
        "  for index, row in walk_distance.iterrows():\n",
        "    if row[\"d_walk\"] < merge_radius:\n",
        "      near_nodes.append((row[\"from_stop_I\"], row[\"to_stop_I\"]))\n",
        "    else:\n",
        "      continue\n",
        "  return near_nodes\n",
        "\n",
        "near_nodes = identify_near_nodes(walking_distance)\n",
        "print(near_nodes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTj5YxRwF2Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_supernodes(G, near_nodes):\n",
        "  \"\"\"\n",
        "  This code creates the supernode network structure\n",
        "  \"\"\" \n",
        "  for u,v in near_nodes:\n",
        "    if u in nx.nodes(G) and v in nx.nodes(G):\n",
        "      G = nx.contracted_nodes(G, u, v, self_loops=False)\n",
        "  return G\n",
        "\n",
        "#create new graphs with supernodes and save them into variables\n",
        "temp_combined_L_graph_sn1112 = create_supernodes(combined_L_1112_graph, near_nodes)\n",
        "temp_combined_L_graph_sn1112 = create_supernodes(combined_L_1112_graph, near_nodes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spy19OmG_Taj",
        "colab_type": "text"
      },
      "source": [
        "Saving the supernode graphs, so that we don't have to run the code every time\n",
        "again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fz5urGwhbMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving the 8am-9am L-space graphs with supernodes into a pickle file\n",
        "nx.write_gpickle(temp_combined_L_graph_sn, DATA_PATH + \"/rome/combined89_L_graph_sn\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3qW4jQTJT7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving the 11am-12am L-space graphs with supernodes into a pickle file\n",
        "nx.write_gpickle(temp_combined_L_graph_sn1112, DATA_PATH + \"/rome/combined1112_L_graph_sn\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBb2uAl7kxOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#uploading the supernode Lspace graphs from the pickle files written in \n",
        "#the code above1112\n",
        "combined89_L_graph_sn = nx.read_gpickle(DATA_PATH + \"/rome/combined89_L_graph_sn\")\n",
        "combined1112_L_graph_sn = nx.read_gpickle(DATA_PATH + \"/rome/combined1112_L_graph_sn\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz8i_57dpEYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uploading graphs into edgelists\n",
        "combined89_L_list_sn = nx.to_pandas_edgelist(combined89_L_graph_sn)\n",
        "combined1112_L_list_sn = nx.to_pandas_edgelist(combined1112_L_graph_sn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVjJ8VfZAAD4",
        "colab_type": "text"
      },
      "source": [
        "## Convert L-space to P-space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPbltQxTh5XB",
        "colab_type": "text"
      },
      "source": [
        "Combined 8am-9am & 11am-12am\n",
        "\n",
        "This code creates the unweighted P-space edgelist, see example Table 5 in paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDZ3PIhfh3k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code below transforms L-space data into P-space data for combined data\n",
        "#Only feasible for combined data\n",
        "def drop_unnecessary_L_data_C89(L_space):\n",
        "  df = L_space.copy()\n",
        "  df[[\"route_I_counts_1\", \"route_I_counts_2\", \"route_I_counts_3\", \"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"]] = df.route_I_counts.str.split(\",\", expand = True) #putting the different route counts into separate columns\n",
        "  df = df.drop(columns = [\"route_I_counts\", \"d\", \"duration_avg\", \"n_vehicles\"]) #drop all unnecessary columns\n",
        "  df['route_I_counts_1'] = df['route_I_counts_1'].str.split(\":\", expand = True) #remove count for routes --> In case I need count later, make a new column for count: df['route_I_1', \"route_I_1_count\"] =\\\n",
        "  df['route_I_counts_2'] = df['route_I_counts_2'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_3'] = df['route_I_counts_3'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_4'] = df['route_I_counts_4'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_5'] = df['route_I_counts_5'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_6'] = df['route_I_counts_6'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_7'] = df['route_I_counts_7'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_8'] = df['route_I_counts_8'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_9'] = df['route_I_counts_9'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_10'] = df['route_I_counts_10'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df['route_I_counts_11'] = df['route_I_counts_11'].str.split(\":\", expand = True) #remove count for routes\n",
        "  df.fillna(value = int(0), inplace = True)\n",
        "  df = df.astype(str).astype(int)\n",
        "  return df\n",
        "\n",
        "def sep_routes_C89(L_space):\n",
        "  routes = drop_unnecessary_L_data_C89(L_space)\n",
        "  \"\"\"\n",
        "  Cleans routes dataframe. Returns a dataframe with stops (u,v) and corresponding route. \n",
        "  Always check maximum amount of separate routes on one line in mode dataset,\n",
        "  adapt the amount of created \"route_n\" variables to that.\n",
        "  \"\"\"\n",
        "  df_new = pd.DataFrame(columns= [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"])\n",
        "  route_1 = routes.loc[routes[\"route_I_counts_1\"] != 0 ]\n",
        "  route_1 = route_1.drop(columns = [\"route_I_counts_2\", \"route_I_counts_3\", \"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_1.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_2 = routes.loc[routes[\"route_I_counts_2\"] != 0 ]\n",
        "  route_2 = route_2.drop(columns = [\"route_I_counts_1\", \"route_I_counts_3\",\"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_2.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_3 = routes.loc[routes[\"route_I_counts_3\"] != 0 ]\n",
        "  route_3 = route_3.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_3.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_4 = routes.loc[routes[\"route_I_counts_4\"] != 0 ]\n",
        "  route_4 = route_4.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_4.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_5 = routes.loc[routes[\"route_I_counts_5\"] != 0 ]\n",
        "  route_5 = route_5.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_5.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_6 = routes.loc[routes[\"route_I_counts_6\"] != 0 ]\n",
        "  route_6 = route_6.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_6.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_7 = routes.loc[routes[\"route_I_counts_7\"] != 0 ]\n",
        "  route_7 = route_7.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_7.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_8 = routes.loc[routes[\"route_I_counts_8\"] != 0 ]\n",
        "  route_8 = route_8.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_8.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_9 = routes.loc[routes[\"route_I_counts_9\"] != 0 ]\n",
        "  route_9 = route_9.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_8\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_9.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_10 = routes.loc[routes[\"route_I_counts_10\"] != 0 ]\n",
        "  route_10 = route_10.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_8\", \"route_I_counts_9\", \"route_I_counts_11\"])\n",
        "  route_10.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "  route_11 = routes.loc[routes[\"route_I_counts_11\"] != 0 ]\n",
        "  route_11 = route_11.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_8\", \"route_I_counts_9\", \"route_I_counts_10\",])\n",
        "  route_11.columns = [\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\"]\n",
        "\n",
        "  pdList = [route_1, route_2, route_3, route_4, route_5, route_6, route_7,\n",
        "            route_8, route_9, route_10, route_11]\n",
        "  df_new = pd.concat(pdList)\n",
        "  df_new = df_new.sort_index()\n",
        "  df_new[\"route_I\"] = df_new[\"route_I\"].astype(str).astype(\"int64\")\n",
        "  df_new = df_new.sort_values(by = [\"route_I\"])\n",
        "  return df_new\n",
        "\n",
        "def create_dict_C89(L_space):\n",
        "  \"\"\"\n",
        "  This transforms the dataset created above into a dictionary with the routes \n",
        "  as keys and the stops on that route as values. For example, route A : [1, 2, 3, 4....]\n",
        "  In the combined dataset the keys consist of two values in a tuple: (route, mode of transport)\n",
        "  \"\"\"\n",
        "  L_space = sep_routes_C89(L_space)\n",
        "  L_space = L_space.groupby([\"route_I\", \"route_type\"])\n",
        "  dict1 =  L_space[\"from_stop_I\"].unique().to_dict()\n",
        "  dict2 =  L_space[\"to_stop_I\"].unique().to_dict()\n",
        "  ds = [dict1, dict2]\n",
        "  d = {}\n",
        "  for k in dict1.keys():\n",
        "    d[k] = np.concatenate(list(d[k] for d in ds))\n",
        "  for k,v in d.items():\n",
        "    d[k] = np.unique(v)\n",
        "  return d\n",
        "\n",
        "def P_space_C89(L_space):\n",
        "  \"\"\"\n",
        "  Converts the dictionary created above into a dataframe consisting of P_space data\n",
        "  \"\"\"\n",
        "  L_space_dict = create_dict_C89(L_space)\n",
        "  df_list = []\n",
        "  for k,v in L_space_dict.items():\n",
        "    for values in v:\n",
        "      for values2 in v:\n",
        "        if values != values2:\n",
        "          df_list.append({\"from_stop_I\": values, \"to_stop_I\": values2, \"route_I\": k[0], \"route_type\": k[1]})\n",
        "        else:\n",
        "          continue\n",
        "  result = pd.DataFrame(df_list)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqBVA0dfXM4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the supernode L_space edgelist to P-space edgelists with the code\n",
        "#written aboven \n",
        "combined89_P_list_sn = P_space_C89(combined89_L_list_sn)\n",
        "combined1112_P_list_sn = P_space_C89(combined1112_L_list_sn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnKjeolZa0Kr",
        "colab_type": "text"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHHXB8L4Sxi",
        "colab_type": "text"
      },
      "source": [
        "## Compute in vehicle travel time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXQnaM_9rqZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_travel_time(mode_P_edgelist, mode_L_graph ):\n",
        "    '''\n",
        "    calculates travel time between all connected stops in P space, used for\n",
        "        supernode data:\n",
        "        mode_P_edgelist = P_space pandas_edgelist with origin and destination \n",
        "        node(stop) and route(line) number. In addition, combined data consists \n",
        "        of route mode column. \n",
        "        mode_L_graph = networkx graph of the L-space of the same mode as \n",
        "        mode_P_edgelist.\n",
        "    returns:\n",
        "        Pandas dataframe (edgelist) consisting of columns origin node and \n",
        "        destination node, route(line) number, route mode when combined data, and\n",
        "        travel time.\n",
        "    '''\n",
        "    mode_shortest_path = dict(nx.all_pairs_dijkstra_path_length(mode_L_graph, weight = \"duration_avg\"))\n",
        "    for row in mode_P_edgelist.itertuples():\n",
        "      mode_P_edgelist.at[row.Index, \"travel_time\"] = mode_shortest_path[row[1]][row[2]]\n",
        "    return mode_P_edgelist\n",
        "\n",
        "\n",
        "\n",
        "combined89_P_list_sn = add_travel_time(combined89_P_list_sn, combined89_L_graph_sn) #be aware: both the P-edgelist and supernode graph need to be up to date\n",
        "combined1112_P_list_sn = add_travel_time(combined1112_P_list_sn, combined1112_L_graph_sn) #be aware: both the P-edgelist and supernode graph need to be up to date\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx4IdUm5lG8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load to new P-space data into csv files, so that the code above doesnt have to be\n",
        "## runned every time again\n",
        "combined89_P_list_sn.to_csv(DATA_PATH + \"/rome/combined89_P_list_sn_tt.csv\", index = False)\n",
        "combined1112_P_list_sn.to_csv(DATA_PATH + \"/rome/combined1112_P_list_sn_tt.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xmsY5f0l5HA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Load the P-space data from the csv files into variables\n",
        "combined89_P_list_sn = pd.read_csv(DATA_PATH + \"/rome/combined89_P_list_sn_tt.csv\")\n",
        "combined1112_P_list_sn = pd.read_csv(DATA_PATH + \"/rome/combined1112_P_list_sn_tt.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLG6u240XlT3",
        "colab_type": "text"
      },
      "source": [
        "## Compute waiting time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcC4ZvytpCNy",
        "colab_type": "text"
      },
      "source": [
        "Combined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXh3u8ldvA2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "The code below creates a new column in the P-space edge list: waiting time.\n",
        "The waiting time is calculated as half of the time between the joint service\n",
        "frequency of an edge.\n",
        "\"\"\"\n",
        "\n",
        "def sep_route_counts_C(L_space):\n",
        "  \"\"\"\n",
        "  This function puts the joint route count per edge in L-space into separate columns\n",
        "  \"\"\" \n",
        "  df = L_space.copy()\n",
        "  df[[\"route_I_counts_1\", \"route_I_counts_2\", \"route_I_counts_3\", \"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"]] = df.route_I_counts.str.split(\",\", expand = True)\n",
        "  df.fillna(value = int(0), inplace = True)\n",
        "  return df\n",
        "\n",
        "def sep_route_C(L_space):\n",
        "  \"\"\"\n",
        "  This function transforms the dataset created above.\n",
        "  Basically, what happens is that every single route on an edge gets a distinct\n",
        "  row. This codes creates an easily readable dataFrame in L-space, necessary to\n",
        "  count the frequency per edge (see next code).\n",
        "  \"\"\"  \n",
        "  routes = sep_route_counts_C(L_space)\n",
        "  df_new = pd.DataFrame(columns= [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"])\n",
        "  route_1 = routes.loc[routes[\"route_I_counts_1\"] != 0 ]\n",
        "  route_1 = route_1.drop(columns = [\"route_I_counts_2\", \"route_I_counts_3\", \"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_1.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_2 = routes.loc[routes[\"route_I_counts_2\"] != 0 ]\n",
        "  route_2 = route_2.drop(columns = [\"route_I_counts_1\", \"route_I_counts_3\",\"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_2.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_3 = routes.loc[routes[\"route_I_counts_3\"] != 0 ]\n",
        "  route_3 = route_3.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_4\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_3.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_4 = routes.loc[routes[\"route_I_counts_4\"] != 0 ]\n",
        "  route_4 = route_4.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_4.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_5 = routes.loc[routes[\"route_I_counts_5\"] != 0 ]\n",
        "  route_5 = route_5.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_6\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_5.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_6 = routes.loc[routes[\"route_I_counts_6\"] != 0 ]\n",
        "  route_6 = route_6.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_7\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_6.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_7 = routes.loc[routes[\"route_I_counts_7\"] != 0 ]\n",
        "  route_7 = route_7.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_8\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_7.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_8 = routes.loc[routes[\"route_I_counts_8\"] != 0 ]\n",
        "  route_8 = route_8.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_9\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_8.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_9 = routes.loc[routes[\"route_I_counts_9\"] != 0 ]\n",
        "  route_9 = route_9.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_8\", \"route_I_counts_10\", \"route_I_counts_11\"])\n",
        "  route_9.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_10 = routes.loc[routes[\"route_I_counts_10\"] != 0 ]\n",
        "  route_10 = route_10.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_8\", \"route_I_counts_9\", \"route_I_counts_11\"])\n",
        "  route_10.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  route_11 = routes.loc[routes[\"route_I_counts_11\"] != 0 ]\n",
        "  route_11 = route_11.drop(columns = [\"route_I_counts_1\", \"route_I_counts_2\",\"route_I_counts_3\",\n",
        "      \"route_I_counts_4\", \"route_I_counts_5\", \"route_I_counts_6\", \"route_I_counts_7\",\n",
        "      \"route_I_counts_8\", \"route_I_counts_9\", \"route_I_counts_10\"])\n",
        "  route_11.columns = [\"from_stop_I\", \"to_stop_I\", \"d\", \"duration_avg\", \"route_type\", \"route_I_counts\", \"n_vehicles\", \"route_I\"]\n",
        "  pdList = [route_1, route_2, route_3, route_4, route_5, route_6, route_7,\n",
        "            route_8, route_9, route_10, route_11]\n",
        "  df_new = pd.concat(pdList)\n",
        "  df_new = df_new.sort_index()\n",
        "  return df_new\n",
        "\n",
        "def route_count_C(L_space):\n",
        "  \"\"\"\n",
        "  This function creates a dataframe consisting of the two adjacent stops of a node,\n",
        "  its route number and the frequency of a particular route on that edge.\n",
        "  In other words, it creates a quick overview of the frequency per edge, per route.\n",
        "  \"\"\" \n",
        "  L_space = sep_route_C(L_space) #Apply the codes written above to the L-space data\n",
        "  L_space = L_space.drop(columns = [\"route_I_counts\", \"d\", \"duration_avg\", \"n_vehicles\"]) #drop all unnecessary columns\n",
        "  L_space[[\"route_I\", \"n_vehicles\"]] = L_space.route_I.str.split(\":\", expand = True) #derive the right route_I and n_vehicles from route_I_counts\n",
        "  L_space[\"n_vehicles\"] = L_space[\"n_vehicles\"].astype(str).astype(\"int64\") #n_vehicles as integer\n",
        "  return L_space\n",
        "\n",
        "def add_waiting_time_C(L_space, P_space):\n",
        "  \"\"\"\n",
        "  By selecting the rouded-up average of the edges belonging to one route, this\n",
        "  function is able to calculate the frequency per route. After calculating the\n",
        "  frequency this function calculates the waiting time (see section 3 in methods\n",
        "  for more explanation) and adds the waiting time in the P-space DataFrame\n",
        "  edgelist.\n",
        "  \"\"\"\n",
        "  P_space = P_space.copy()\n",
        "  L_space = route_count_C(L_space) #Apply the codes written above to the L-space data\n",
        "  frequency_dict =  np.ceil(L_space.groupby([\"route_I\"]).n_vehicles.mean()).to_dict() #retrieve the frequency of routes and place them into a dict\n",
        "  #This is done by calculating the ceiled mean of the number of vehicles that drive between nodes, separated by route\n",
        "\n",
        "  for row in P_space.itertuples():\n",
        "    for k,v in frequency_dict.items():\n",
        "      if row[3] == int(k): #if route_i is similar to route_i in dictionary keys\n",
        "        P_space.at[row.Index, \"frequency\"] = int(v) #Creates the column frequency in P-space which consists of the frequency per route as calculated in frequency_dict\n",
        "\n",
        "  P_space['route_I'] = P_space['route_I'].astype(str)\n",
        "  P_space = P_space.groupby(['from_stop_I', 'to_stop_I', \"route_type\", \"travel_time\"]).agg({\"frequency\": \"sum\",\n",
        "                                                                    'route_I': ','.join}).reset_index() \n",
        "  #sums the frequency per edge in P-space and transfers multi edge into single edge with the join function.\n",
        "  #In other words: if two adjacent nodes are connected by more than one route of the same mode of transport, the different routes are merged\n",
        "\n",
        "  P_space[\"waiting_time\"] = (60*60)/(2*P_space[\"frequency\"]) #create new column waiting time: 3600 seconds (timespan used) divided by 2 times the headway\n",
        "  P_space = P_space[[\"from_stop_I\", \"to_stop_I\", \"route_type\", \"route_I\", \"frequency\", \"travel_time\", \"waiting_time\"]] #rearrange column order\n",
        "\n",
        "  return P_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjAyopRtxRCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Apply code\n",
        "combined89_P_sn = add_waiting_time_C(combined89_L_list_sn, combined89_P_list_sn )\n",
        "combined1112_P_sn = add_waiting_time_C(combined1112_L_list_sn, combined1112_P_list_sn )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM8VVNANg2kP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The code below saves the new P-space data set (including travel time and waiting\n",
        "#as separate columns), in order to prevent that we have to run the code everytime again\n",
        "combined89_P_sn.to_csv(DATA_PATH + \"/rome/combined89_P_sn_ttwt.csv\", index = False)\n",
        "combined1112_P_sn.to_csv(DATA_PATH + \"/rome/combined1112_P_sn_ttwt.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnLlBbVkHSOT",
        "colab_type": "text"
      },
      "source": [
        "## Compute weight (consisting of waiting time and travel time)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAKe8P_zP5__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading data saved above\n",
        "combined89_P_sn = pd.read_csv(DATA_PATH + \"/rome/combined89_P_sn_ttwt.csv\")\n",
        "combined1112_P_sn = pd.read_csv(DATA_PATH + \"/rome/combined1112_P_sn_ttwt.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXqTffN2kBDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_weight(P_space):\n",
        "  \"\"\"\n",
        "  Adds the waiting time to the in-vehicle travel time and creates a new column\n",
        "    in the dataset comprising the total travel time which can be used as \n",
        "    weight for the edges in the network representation.\n",
        "  returns:\n",
        "    Pandas dataframe (edgelist) with extra column \"weight\"\n",
        "  \"\"\"\n",
        "  P_space[\"weight\"] = P_space[\"travel_time\"] + P_space[\"waiting_time\"]\n",
        "  return P_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYVwv4CR3uLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#applying compute_weight function\n",
        "combined89_P_sn = compute_weight(combined89_P_sn)\n",
        "combined1112_P_sn = compute_weight(combined1112_P_sn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKZ6K1uunNRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_multigraph_data(P_space):\n",
        "  \"\"\"\n",
        "  Important note: this step is only needed for a multimodal dataset (for single-modal datasets: nx.Graph will do the same)\n",
        "  Seen the fact that there are duplicate edges in the data (edges wit same source and target nodes, but different modes of transport)\n",
        "    We need a multigraph (data set) that makes a distinction in the two different modes of transport between two adjacent stops (in case they exist).\n",
        "    This code converts the P_space data set with all edge attributes into a database that can directly be implemented as a multigraph\n",
        "    in the shortest path analyzer needed to calculate the generalised travel costs per link in P_space.\n",
        "    For the database, this means, firstly, that we get rid of directed edges and we convert them into undirected edges. This is a necessary step to take\n",
        "    since we make use of a supernode graph structure (nodes on <=100 m walking distance have been contracted (e.g. nodes on opposite side of the road)). \n",
        "    Secondly, since we cannot use a Graph (we need a Multigraph for edges between the same pair of nodes, but with a different mode of transport),\n",
        "    we have to remove duplicate edges manually (this are edges that were directed first, but double in an undirected multigraph) \n",
        "  returns:\n",
        "    Pandas dataframe (edgelist) ready to use for the shortest path analyer that calculates the travel generalised travel costs per link in P_space\n",
        "  \"\"\"\n",
        "  P_space = P_space.copy()\n",
        "  P_space = nx.from_pandas_edgelist(P_space, source = \"from_stop_I\", target = \"to_stop_I\", \n",
        "            edge_attr = [\"route_I\", \"route_type\", \"frequency\", \"travel_time\", \"waiting_time\", \"weight\"], create_using = nx.MultiGraph())\n",
        "  P_space = nx.to_pandas_edgelist(P_space)\n",
        "  P_space = P_space.drop_duplicates()\n",
        "  P_space = P_space.reset_index(drop = True)\n",
        "  return P_space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBradGOZoFYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#applying to_multigraph_data function\n",
        "combined89_P_sn = to_multigraph_data(combined89_P_sn)\n",
        "combined1112_P_sn = to_multigraph_data(combined1112_P_sn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Xbdz9ikJ2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save dataset\n",
        "combined89_P_sn.to_csv(DATA_PATH + \"/rome/combined89_P_sn_weight.csv\", index = False)\n",
        "combined1112_P_sn.to_csv(DATA_PATH + \"/rome/combined1112_P_sn_weight.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivyGLh6ctYNv",
        "colab_type": "text"
      },
      "source": [
        "# Convert P-edgelist to graph\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGAnaHBNo3nm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data\n",
        "combined89_P_sn = pd.read_csv(DATA_PATH + \"/rome/combined89_P_sn_weight.csv\")\n",
        "combined1112_P_sn = pd.read_csv(DATA_PATH + \"/rome/combined1112_P_sn_weight.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKlvuS8dt3iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert the edgelist into a graph ready to use for the shortest path analyzer to generate generalised travel costs between node pairs\n",
        "rome_89_P_graph = nx.from_pandas_edgelist(combined89_P_sn, source = \"source\", target = \"target\", \n",
        "            edge_attr = [\"route_I\", \"route_type\", \"frequency\", \"travel_time\", \"waiting_time\", \"weight\"], create_using = nx.MultiGraph())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVaPwAjIjIpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert the edgelist into a graph ready to use for the shortest path analyzer to generate generalised travel costs between node pairs\n",
        "rome_1112_P_graph = nx.from_pandas_edgelist(combined1112_P_sn, source = \"source\", target = \"target\", \n",
        "            edge_attr = [\"route_I\", \"route_type\", \"frequency\", \"travel_time\", \"waiting_time\", \"weight\"], create_using = nx.MultiGraph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-u7IUUNUUL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add coordinates and node names as node attributes to the graph\n",
        "network_nodes['coord'] = list(zip(network_nodes.lon, network_nodes.lat))\n",
        "pos = network_nodes.set_index(\"stop_I\")[\"coord\"].to_dict()\n",
        "\n",
        "def add_coordinates(network_nodes, G):\n",
        "  \"\"\"\n",
        "    network_nodes = csv file with network node coordinates\n",
        "    G = networkx (multi)graph\n",
        "  This function adds the coordinates to the nodes in the selected graph\n",
        "  returns: networkx (multi)graph \n",
        "  \"\"\"\n",
        "  network_nodes['coord'] = list(zip(network_nodes.lon, network_nodes.lat))\n",
        "  pos = network_nodes.set_index(\"stop_I\")[\"coord\"].to_dict()\n",
        "  nx.set_node_attributes(G, pos, name = \"pos\")\n",
        "  return G\n",
        "\n",
        "def add_name(network_nodes, G):\n",
        "  \"\"\"\n",
        "    network_nodes = csv file with network node coordinates\n",
        "    G = networkx (multi)graph\n",
        "  This function adds the name of the stops to the nodes in the selected graph\n",
        "  returns: networkx (multi)graph \n",
        "  \"\"\"\n",
        "  name = network_nodes.set_index(\"stop_I\")[\"name\"].to_dict()\n",
        "  nx.set_node_attributes(G, name, \"name\" )\n",
        "  return G\n",
        "\n",
        "rome_89_P_graph = add_coordinates(network_nodes, rome_89_P_graph)\n",
        "rome_89_P_graph = add_name(network_nodes, rome_89_P_graph)\n",
        "rome_89_P_graph.nodes(data= True) #should reflect pos and name per node \n",
        "rome_1112_P_graph = add_coordinates(network_nodes, rome_1112_P_graph)\n",
        "rome_1112_P_graph = add_name(network_nodes, rome_1112_P_graph)\n",
        "rome_1112_P_graph.nodes(data= True) #should reflect pos and name per node "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_DWjYAwqZV0",
        "colab_type": "text"
      },
      "source": [
        "# Select giant connected component\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUff7LvqXoX0",
        "colab_type": "text"
      },
      "source": [
        "Below we select the giant connected component\n",
        "In Rome there is a small part of the network (26 nodes out of 5419 in total) that is not connected, for the accessibility analysis these are not included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p69ESAVj1SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connected_component_subgraphs(G):\n",
        "  \"\"\"\n",
        "  This function selects the connected components in a graph\n",
        "  \"\"\"\n",
        "  for c in nx.connected_components(G):\n",
        "      yield G.subgraph(c)\n",
        "\n",
        "def subgraph_check(G):\n",
        "  \"\"\"\n",
        "  This function puts the nodes and edges of the subgraphs into dictionaries.\n",
        "  Furthermore, this function prints the number of nodes and edges per subgraph\n",
        "  Input: nx.Graph\n",
        "  Returns: two dictionaries. First consists of subgraphs and corresponding nodes.\n",
        "  Second consists of subgraphs and corresponding edges.\n",
        "  \"\"\"\n",
        "  count = 1\n",
        "  subgraph_nodes_dict = dict()\n",
        "  subgraph_edges_dict = dict()\n",
        "  for g in connected_component_subgraphs(G):\n",
        "    print(\"number of nodes {}:\".format(count), g.number_of_nodes())\n",
        "    nodes_list = list(g.nodes)\n",
        "    subgraph_nodes_dict[count] = nodes_list\n",
        "    print(\"number of edges {}:\".format(count), g.number_of_edges())\n",
        "    edges_list = list(g.nodes)\n",
        "    subgraph_edges_dict[count] = edges_list = list(g.nodes)\n",
        "    count += 1\n",
        "  return subgraph_nodes_dict, subgraph_edges_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLpvFAHvqTKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nodes_subgraph_rome89, edges_subgraph_rome89 = subgraph_check(rome_89_P_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpR60IqOlhBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nodes_subgraph_rome1112, edges_subgraph_rome1112 = subgraph_check(rome_1112_P_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhPU3P-N1FGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## This code puts the subgraphs into separate variables, be aware of the fact that the right amount of \n",
        "## variables has been used before the \"=\"\n",
        "\n",
        "rome_89_largest_cc, rome_89_smaller_cc_1 = [rome_89_P_graph.subgraph(c).copy() for c in nx.connected_components(rome_89_P_graph)]\n",
        "##The smaller connected component is a busroute not connected to the rest of the network, see subgraph_detection function for more information"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zNSyNsnl8Fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code puts the subgraphs into separate variables, be aware of the fact that the right amount of \n",
        "# variables has been used before the \"=\"\n",
        "\n",
        "rome_1112_largest_cc, rome_1112_smaller_cc_1, rome_1112_smaller_cc_2 = [rome_1112_P_graph.subgraph(c).copy() for c in nx.connected_components(rome_1112_P_graph)]\n",
        "#The smaller connected components are: (1) rome_1112_smaller_cc_1, a railroute (331 & 335) not connected to the rest of the network\n",
        "#(2) rome_89_smaller_cc_1, a busroute not connected to the rest of the network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DLM2tSE8noK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save GCC\n",
        "nx.write_gpickle(rome_1112_largest_cc, DATA_PATH + \"/rome/rome1112_P_GCC\")\n",
        "nx.write_gpickle(rome_89_largest_cc, DATA_PATH + \"/rome/rome89_P_GCC\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOF_jtA0LWar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load GCC\n",
        "rome_89_largest_cc = nx.read_gpickle(DATA_PATH + \"/rome/rome89_P_GCC\")\n",
        "rome_1112_largest_cc = nx.read_gpickle(DATA_PATH + \"/rome/rome1112_P_GCC\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOwRpkARfH42",
        "colab_type": "text"
      },
      "source": [
        "# Compute generalised travel costs for every node-pair in the network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnib8gqlGEmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_GTC_matrix(G):\n",
        "  \"\"\"\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: three dataframe matrices. (1) transfer penalty time\n",
        "  (2) travel time (consisting of in vehicle travel time and waiting time)\n",
        "  (3) Sum of both: generalized travel costs\n",
        "  \"\"\"\n",
        "  dijkstra_path_length = dict(nx.all_pairs_dijkstra(G, weight = \"weight\"))\n",
        "  length = dict()\n",
        "  path = dict()\n",
        "  for key, values in dijkstra_path_length.items():\n",
        "    length[key] = values[0]\n",
        "    path[key] = values[1]\n",
        "\n",
        "  \"\"\"\n",
        "  The code below calculates the number of transfers made from all to all nodes\n",
        "  with all_pairs_dijkstra_shortest_path\n",
        "\n",
        "  \"\"\"\n",
        "  df_path = pd.DataFrame(path)\n",
        "  df_path = df_path.astype('str')\n",
        "  df_path = df_path.replace('\\[','', regex = True, inplace=False)\n",
        "  df_path = df_path.replace(']','', regex = True, inplace = False)\n",
        "  count_transfer = []\n",
        "  for index, row in df_path.iterrows():\n",
        "    A = row.str.count(',')\n",
        "    count_transfer.append(A)\n",
        "  df_transfer = pd.DataFrame(count_transfer)\n",
        "\n",
        "  \"\"\"\n",
        "  The code below, transfers are being penalized with a 300 seconds (5 minutes) penalty\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with count of transfers\n",
        "  \"\"\"\n",
        "  \n",
        "  transfer_penalty = lambda x: 300*(x-1)\n",
        "  df_transfer = transfer_penalty(df_transfer)\n",
        "  df_transfer[df_transfer == (-300)] = 0 \n",
        "  df_transfer = df_transfer.sort_index(axis=0)\n",
        "  df_transfer = df_transfer.sort_index(axis=1)\n",
        "  \n",
        "  \"\"\"\n",
        "  Calculates the travel time consisting of in-vehicle travel time and waiting time\n",
        "  with all_pairs_dijkstra_shortest_path\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with travel time\n",
        "  \"\"\"\n",
        "  travel_time = length\n",
        "  df_travel = pd.DataFrame(length)             \n",
        "  df_travel = df_travel.sort_index(axis=0)\n",
        "  df_travel = df_travel.sort_index(axis=1)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  The code below calculates the Generalised Travel Costs consisting of in-vehicle travel time,\n",
        "  waiting time and transfer penalty with all_pairs_dijkstra_shortest_path\n",
        "  In line with method developed by Luo et al. 2019 (see thesis report for complete\n",
        "  reference)\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with GTC\n",
        "  \"\"\" \n",
        "\n",
        "  df_GTC = df_travel.add(df_transfer, fill_value=0)\n",
        "  df_GTC = df_GTC.sort_index(axis=0)\n",
        "  df_GTC = df_GTC.sort_index(axis=1)\n",
        "\n",
        "\n",
        "  return df_transfer, df_travel, df_GTC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8KqbKPBopRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply code and save matrices\n",
        "rome_1112_transfer, rome_1112_travel, rome_1112_gtc = generate_GTC_matrix(rome_1112_largest_cc)\n",
        "rome_1112_gtc.to_csv(DATA_PATH + \"/rome1112_final_gtc.csv\", index = True)\n",
        "rome_1112_transfer.to_csv(DATA_PATH + \"/rome1112_final_transfer.csv\", index = True)\n",
        "rome_1112_travel.to_csv(DATA_PATH + \"/rome1112_final_travel.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk3JmOLouTQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply code and save matrices\n",
        "rome_89_transfer, rome_89_travel, rome_89_gtc = generate_GTC_matrix(rome_89_largest_cc)\n",
        "rome_89_gtc.to_csv(DATA_PATH + \"/rome89_final_gtc.csv\", index = True)\n",
        "rome_89_transfer.to_csv(DATA_PATH + \"/rome89_final_transfer.csv\", index = True)\n",
        "rome_89_travel.to_csv(DATA_PATH + \"/rome89_final_travel.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avjNxqQVwbk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load GTC matrix 8-9AM\n",
        "rome_89_gtc = pd.read_csv(DATA_PATH + \"/rome89_final_gtc.csv\", sep=\",\")\n",
        "rome_89_gtc = rome_89_gtc.set_index(\"Unnamed: 0\")\n",
        "rome_89_gtc.index.name = None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By8cGfAX_g8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load GTC matrix 8-9AM\n",
        "rome_1112_gtc = pd.read_csv(DATA_PATH + \"/rome1112_final_gtc.csv\", sep=\",\")\n",
        "rome_1112_gtc = rome_1112_gtc.set_index(\"Unnamed: 0\")\n",
        "rome_1112_gtc.index.name = None\n",
        "rome_1112_gtc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbknSx73APv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate Travel Impedance per node in 11-12AM data\n",
        "rome_1112_travel_impedance = rome_1112_gtc.sum()/5375  #sum/(sum-1)\n",
        "rome_1112_travel_impedance = pd.DataFrame(rome_1112_travel_impedance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxNoRMlV6DUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate Travel Impedance per node in 8-9AM data\n",
        "rome_89_travel_impedance = rome_89_gtc.sum()/5397 #sum/(sum-1)\n",
        "rome_89_travel_impedance = pd.DataFrame(rome_89_travel_impedance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4AFYQgVKafR",
        "colab_type": "text"
      },
      "source": [
        "# Compute in-vehicle travel time for every node-pair in the P-space network representation (needed for violin plot section 4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggpZnFfkdsSy",
        "colab_type": "text"
      },
      "source": [
        "The code below calculates the in-vehicle travel time for every node pair, in exactly the same way as how the GTC has been calculated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jv4cFKJKhaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_IVTT_matrix(G):\n",
        "  \"\"\"\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: pd.DataFrame matrix with total in vehicle travel time as gtc-component\n",
        "  \"\"\"\n",
        "  length = dict(nx.all_pairs_dijkstra_path_length(G, weight = \"travel_time\"))\n",
        "\n",
        "  \"\"\"\n",
        "  Calculates the travel time consisting of in-vehicle travel time and waiting time\n",
        "  with all_pairs_dijkstra_shortest_path\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with travel time\n",
        "  \"\"\"\n",
        "  df_travel = pd.DataFrame(length)             \n",
        "  df_travel = df_travel.sort_index(axis=0)\n",
        "  df_travel = df_travel.sort_index(axis=1)\n",
        "\n",
        "  return df_travel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvHvs5HgL04G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome89_IVTT = generate_IVTT_matrix(rome_89_largest_cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWveuxCJXEbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome112_IVTT = generate_IVTT_matrix(rome_1112_largest_cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFabsRGleLNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome89_IVTT.to_csv(DATA_PATH + \"/rome/rome89_IVTT.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FhZtE-jeMCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome112_IVTT.to_csv(DATA_PATH + \"/rome/rome1112_IVTT.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdrKNwIRXoOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome89_IVTT = pd.read_csv(DATA_PATH + \"/rome/rome89_IVTT.csv\", sep=\",\")\n",
        "rome89_IVTT = rome89_IVTT.set_index(\"Unnamed: 0\")\n",
        "rome1112_IVTT = pd.read_csv(DATA_PATH + \"/rome/rome1112_IVTT.csv\", sep=\",\")\n",
        "rome1112_IVTT = rome1112_IVTT.set_index(\"Unnamed: 0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRiyGwXlV6Sm",
        "colab_type": "text"
      },
      "source": [
        "# Compute waiting + transfer time for every node pair in the P-space network representation (needed for violin plot section 4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C5chFxdd_hY",
        "colab_type": "text"
      },
      "source": [
        "The code below calculates the waiting time + transfer penalty for every node pair, in exactly the same way as how the GTC has been calculated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWMqqkiSV4ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_WT_matrix(G):\n",
        "  \"\"\"\n",
        "  input: Graph with edge attribute \"waiting_time\"\n",
        "  returns: pd.DataFrame matrix with total waiting and transfer time as gtc-component\n",
        "  \"\"\"\n",
        "  dijkstra_path_length = dict(nx.all_pairs_dijkstra(G, weight = \"waiting_time\"))\n",
        "  length = dict()\n",
        "  path = dict()\n",
        "  for key, values in dijkstra_path_length.items():\n",
        "    length[key] = values[0]\n",
        "    path[key] = values[1]\n",
        "\n",
        "  \"\"\"\n",
        "  The code below calculates the number of transfers made from all to all nodes\n",
        "  with all_pairs_dijkstra_shortest_path\n",
        "\n",
        "  \"\"\"\n",
        "  df_path = pd.DataFrame(path)\n",
        "  df_path = df_path.astype('str')\n",
        "  df_path = df_path.replace('\\[','', regex = True, inplace=False)\n",
        "  df_path = df_path.replace(']','', regex = True, inplace = False)\n",
        "  count_transfer = []\n",
        "  for index, row in df_path.iterrows():\n",
        "    A = row.str.count(',')\n",
        "    count_transfer.append(A)\n",
        "  df_transfer = pd.DataFrame(count_transfer)\n",
        "\n",
        "  \"\"\"\n",
        "  The code below, transfers are being penalized with a 300 seconds (5 minutes) penalty\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with count of transfers\n",
        "  \"\"\"\n",
        "  \n",
        "  transfer_penalty = lambda x: 300*(x-1)\n",
        "  df_transfer = transfer_penalty(df_transfer)\n",
        "  df_transfer[df_transfer == (-300)] = 0 \n",
        "  df_transfer = df_transfer.sort_index(axis=0)\n",
        "  df_transfer = df_transfer.sort_index(axis=1)\n",
        "  \n",
        "  \"\"\"\n",
        "  Calculates the travel time consisting of in-vehicle travel time and waiting time\n",
        "  with all_pairs_dijkstra_shortest_path\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with travel time\n",
        "  \"\"\"\n",
        "  df_waiting = pd.DataFrame(length)             \n",
        "  df_waiting = df_waiting.sort_index(axis=0)\n",
        "  df_waiting = df_waiting.sort_index(axis=1)\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  The code below calculates the Generalised Travel Costs consisting of in-vehicle travel time,\n",
        "  waiting time and transfer penalty with all_pairs_dijkstra_shortest_path\n",
        "  In line with method developed by Luo et al. 2019 (see thesis report for complete\n",
        "  reference)\n",
        "  input: Graph with edge attribute \"weight\"\n",
        "  returns: dataframe matrix with travel time\n",
        "  \"\"\" \n",
        "\n",
        "  df_wandt = df_waiting.add(df_transfer, fill_value=0)\n",
        "  df_wandt = df_wandt.sort_index(axis=0)\n",
        "  df_wandt = df_wandt.sort_index(axis=1)\n",
        "\n",
        "  return df_wandt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZR3wxDba6mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome89_WT = generate_WT_matrix(rome_89_largest_cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBKsG55XbHw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome1112_WT = generate_WT_matrix(rome_1112_largest_cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ9vqyCJbKpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome89_WT.to_csv(DATA_PATH + \"/rome/rome89_WT.csv\", index = True)\n",
        "rome1112_WT.to_csv(DATA_PATH + \"/rome/rome1112_WT.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXT0Y0WxYyzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome1112_WT.to_csv(DATA_PATH + \"/rome/rome1112_WT.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBXV26J8CvTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome89_WT = pd.read_csv(DATA_PATH + \"/rome/rome89_WT.csv\", sep=\",\")\n",
        "rome89_WT = rome89_WT.set_index(\"Unnamed: 0\")\n",
        "rome89_WT.index.name = None\n",
        "rome1112_WT = pd.read_csv(DATA_PATH + \"/rome/rome1112_WT.csv\", sep=\",\")\n",
        "rome1112_WT = rome1112_WT.set_index(\"Unnamed: 0\")\n",
        "rome1112_WT.index.name = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1EEeJC37BI-",
        "colab_type": "text"
      },
      "source": [
        "# Benchmark metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK8fdu3bHwgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#With the codes below we select the giant connected component, which also has been selected in the P_space (space of infrastructure) Graph\n",
        "combined89_L_largest_cc, combined89_smaller_cc_1 = [combined89_L_graph_sn.subgraph(c).copy() for c in nx.connected_components(combined89_L_graph_sn)]\n",
        "combined1112_L_largest_cc, combined1112_L_smaller_cc_1, combined1112_L_smaller_cc_2 = [combined1112_L_graph_sn.subgraph(c).copy() for c in nx.connected_components(combined1112_L_graph_sn)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Tb9yEI6_Gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def topological_benchmark_matrix(G):\n",
        "  \"\"\"\n",
        "  Computes the topological shortest path computed in unweighted L_space\n",
        "  (infrastructural representation). This serves as the benchmark matrix.\n",
        "  From this matrix the travel impedance per node can be derived. The travel\n",
        "  impedance is here the number of links traversed in the infrastructural re-\n",
        "  presentation.\n",
        "  Input: L_space nx.Graph\n",
        "  Returns: Pandas Dataframe\n",
        "  \"\"\"\n",
        "  dijkstra_path_length = dict(nx.all_pairs_dijkstra_path_length(G))\n",
        "  df_length = pd.DataFrame(dijkstra_path_length)\n",
        "  df_length = df_length.sort_index(axis=0)\n",
        "  df_length = df_length.sort_index(axis=1)\n",
        "  return df_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mYvswy38YSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome_89_bm = topological_benchmark_matrix(combined89_L_largest_cc)\n",
        "rome_1112_bm = topological_benchmark_matrix(combined1112_L_largest_cc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVAtKcne_gMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the dataframes to csv files\n",
        "rome_1112_bm.to_csv(DATA_PATH + \"/rome1112_benchmark.csv\", index = True)\n",
        "rome_89_bm.to_csv(DATA_PATH + \"/rome89_benchmark.csv\", index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcyz1OXa_g1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the data from the csv files into a dataframe\n",
        "rome_89_bm = pd.read_csv(DATA_PATH + \"/rome89_benchmark.csv\", sep=\",\")\n",
        "rome_89_bm = rome_89_bm.set_index(\"Unnamed: 0\")\n",
        "rome_89_bm.index.name = None\n",
        "rome_1112_bm = pd.read_csv(DATA_PATH + \"/rome1112_benchmark.csv\", sep=\",\")\n",
        "rome_1112_bm = rome_1112_bm.set_index(\"Unnamed: 0\")\n",
        "rome_1112_bm.index.name = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8uPyppmCOzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Computing the topological travel impedance in L_space infrastructure (benchmark metric)\n",
        "rome_89_topo_ti = rome_89_bm.sum()/5397\n",
        "rome_89_topo_ti = pd.DataFrame(rome_89_topo_ti)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tcea3Vv_PmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rome_1112_topo_ti = rome_1112_bm.sum()/5375\n",
        "rome_1112_topo_ti = pd.DataFrame(rome_1112_topo_ti)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}